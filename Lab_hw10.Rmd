---
title: "Lab HW10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(microbenchmark)
library(tidyverse)
```

# 1. Write a function that calculates cumulative distribution function of a binomial random variable. Compare results from your function with pbinom() function.
```{r}
n <- 100
p <- 0.5
B <- 100000
q <- 50

set.seed(123)
pbinom_manual <- function(q) {
    round(sum(rbinom(B, n, p) <= q) / B, 4)
}

pbinom_manual(q)
round(pbinom(q, n, p), 4)
```
The results using the two methods are very close.

# 2. Write a function that runs simulations to obtain power from a one-sample t-test. Run your function (with number of simulations = 10,000 ) with n = 30, delta = 0.5, sd = 1 and sig.level = 0.05. Compare your results with power.t.test(n = 30, delta = 0.5, sd = 1, sig.level = 0.05, type = ‘one.sample’).
```{r}
B <- 10000
set.seed(123)
power_t_manual <- replicate(B, {
    n <- 30
    x <- rnorm(n, 0.5, 1)
    se <- sd(x) / sqrt(n)
    t <- abs(mean(x) - 0) / se 
    power1 <- ifelse(2 * (1 - pt(t, df = n)) < 0.05, 1, 0)
    power1 
})
power2 <- power.t.test(n = 30, delta = 0.5, sd = 1, sig.level = 0.05, type = "one.sample")$power
sum(power_t_manual) / B
round(power2, 4)
```
The results using the two methods are very close.

# Activity 11
# 1. Write a function that generates numbers from binomial(n, p) distribution using runif() function. Hint: binomial(n, p) random variable can be defined as a sum of n independent Bernoulli(p) random variables.
```{r}
n <- 100
p <- 0.5
B <- 10000
set.seed(123)
sample1 <- replicate(B, {
    sum(ifelse(runif(n, 0, 1) > p, 1, 0))
})
```

#2. Compare performance of your function with rbinom() using microbenchmark() function.
```{r}
set.seed(123)
sample2 <- rbinom(B, n, p)
# compare the summary
summary(sample1)
summary(sample2)
# use microbenchmark
microbenchmark(sample1, sample2)
```

#3. Suppose we want to simulate data from a linear regression model: Fit a linear regression model and plot fitted values vs residuals using ggplot() function. Please do not forget to use set.seed() function for reproducibility.
```{r}
N <- 50
set.seed(123)
x <- sample(seq(from = 20, to = 40, by = 0.1), N, replace=TRUE) 
y <- 15 + (0.4 * x) + rnorm(N, 0, 3)
fit <- lm(y ~ x)
fit_df = data.frame(fitted = fit$fitted.values, res = fit$residuals)
ggplot(data = fit_df, aes(x = fitted, y = res)) +
    geom_point() +
    xlab("Fitted values") +
    ylab("Residuals") +
    ggtitle("A linear regression model")
```

#4.Write a function that generates normal variates using Box-Muller algorithm. Compare simulated data from your function with simulated data from rnorm() function using ggplot() (histogram?).
```{r}
set.seed(123)
n <- 1000
R <- sqrt(-2 * log(runif(n, 0, 1)))
theta <- 2 * pi * runif(n, 0, 1)
X <- R * cos(theta)
Y <- R * sin(theta)
sample1 <- c(X, Y)
sample2 <- rnorm(2 * n, 0, 1)
ggplot() +
    geom_histogram(aes(sample1), fill = "chocolate", alpha = 0.2, bins = 10) +
    geom_histogram(aes(sample2), fill = "blue", alpha = 0.2, bins = 10) +
    xlab("Box-Muller and rnorm Method")
```
We can see that the two histgrams are very similar.